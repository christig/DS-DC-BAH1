{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Practice Loading and Describing Data \n",
    "\n",
    "_Authors: Matt Brems (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "In this lab you will practice loading data using python and describing it with statistics.\n",
    "\n",
    "It might be a good idea to first check the [source of the Boston housing data](https://archive.ics.uci.edu/ml/datasets/Housing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the boston housing data (provided below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data file does not contain the column names in the first line, so we'll need to add those in manually. You can find the names and explanations [here](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names). We've extracted the names below for your convenience. You may choose to edit the names, should you decide it would be more helpful to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\",\n",
    "         \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. CRIM      - per capita crime rate by town\n",
    "# 2. ZN        - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "# 3. INDUS     - proportion of non-retail business acres per town\n",
    "# 4. CHAS      - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "# 5. NOX       - nitric oxides concentration (parts per 10 million)\n",
    "# 6. RM        - average number of rooms per dwelling\n",
    "# 7. AGE       - proportion of owner-occupied units built prior to 1940\n",
    "# 8. DIS       - weighted distances to five Boston employment centres\n",
    "# 9. RAD       - index of accessibility to radial highways\n",
    "# 10. TAX      - full-value property-tax rate per 10,000 dollars. \n",
    "# 11. PTRATIO  - pupil-teacher ratio by town\n",
    "# 12. B        - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "# 13. LSTAT    - Percent lower status of the population\n",
    "# 14. MEDV     - Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the `housing.data` file with python\n",
    "\n",
    "Using any method of your choice.\n",
    "> _**Hint:** despite this file having a strange `.data` extension, using python's `open() as file` and `file.read()` or `file.readlines()` we can load this in and see that it is a text file formatted much the same as a CSV. You can use string operations to format the data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading in the local file 'housing.data'\n",
    "data = []\n",
    "with open('./datasets/housing.data', 'rU') as f:\n",
    "    rows = f.readlines()\n",
    "    for row in rows:\n",
    "        row = [float(x) for x in row.split()]\n",
    "        data.append(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at the first two rows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put the data into a dictionary with keys identified by column names:\n",
    "d = {key_name:[row[index] for row in data] for index, key_name in enumerate(names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first ten values in the 'NOX' key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the keys of the dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Conduct a brief integrity check of your data. \n",
    "\n",
    "This integrity check should include, but is not limited to, checking for missing values and making sure all values make logical sense. (i.e. is one variable a percentage, but there are observations above 100%?)\n",
    "\n",
    "Summarize your findings in a few sentences, including what you checked and, if appropriate, any \n",
    "steps you took to rectify potential integrity issues.\n",
    "\n",
    "_**Not sure what to look for or where to start?**  _\n",
    "\n",
    "**Value Integrity**  \n",
    "A couple of our features, such as `CHAS`, `ZN` and `PTRATIO`, exist on a normalized scale(0-1 or 0-100).  We can iterate through these features and make sure all their associate values are within their respective scales.\n",
    "\n",
    "**Equal observations**  \n",
    "All of our features should have an equal length/ number of observations in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking for features with improperly recorded observations:\n",
    "    \n",
    "# Given the information about the features and observing a few of their \n",
    "# observations I believe there are 7 features which take place on a \n",
    "# normalized scale (0-1 or 0-100)\n",
    "\n",
    "# - CHAS (0-1),\n",
    "# - CRIM(0-100),\n",
    "# - ZN(0-100), \n",
    "# - INDUS(0-100), \n",
    "# - RM(0-100), \n",
    "# - LSTAT(0-100), \n",
    "# - PTRATIO(0-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. For what two attributes does it make the *least* sense to calculate mean and median? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build a function that takes a feature as an input an returns the following calculated by hand (don't use any libraries)!\n",
    "\n",
    "- Mean\n",
    "- Median\n",
    "- Standard Deviation\n",
    "- Variance\n",
    "\n",
    "Run your function on several (or all) of the features.  Report any intereseting findings.\n",
    "\n",
    "_`math.sqrt` is allowed._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math.sqrt\n",
    "\n",
    "def summary(feature):\n",
    "# A:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a Check the results of your equation against numpy.\n",
    "`np.mean(), np.median(), np.std(), np.var()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Which two variables have the strongest linear association? \n",
    "\n",
    "<img src=\"../assets/images/correlation.png\" width=\"600\">\n",
    "\n",
    "Report both variables, the metric you chose as the basis for your comparison, and the value of that metric.  You can use Numpy's [`np.corrcoef`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.corrcoef.html) to calculate the Pearson R Correlation Coefficient. \n",
    "*(Hint: Make sure you consider only variables for which it makes sense to find a linear association.)*\n",
    "\n",
    "\n",
    "_Example_\n",
    "```python\n",
    "np.corrcoef(d['CHAS'],d['CRIM'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Look at distributional qualities of variables.\n",
    "\n",
    "<img src=\"../assets/images/distributions.jpg\" width=\"500\">\n",
    "\n",
    "Answer the following questions:\n",
    "1. Which variable has the most symmetric distribution? \n",
    "    - *Symmetric: the metric with the smallest abs(mean - median)*\n",
    "2. Which variable has the most left-skewed (negatively skewed) distribution? \n",
    "    - *Left: the metric with the smallest mean - median*\n",
    "3. Which variable has the most right-skewed (positively skewed) distribution? \n",
    "    - *Right: the metric with the largest mean - median*\n",
    "    \n",
    "Defend your method for determining this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Symmetric: the metric with the smallest abs(mean - median)\n",
    "# Left: the metric with the smallest mean - median\n",
    "# Right: the metric with the largest mean - median\n",
    "\n",
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When it comes to assessing distributions normalizing and scaling is wildly important when looking at the true distributions of the data.**\n",
    "\n",
    "### Optional: Repeat question 7 but scale the variables by their range first.\n",
    "\n",
    "Scale = $\\sigma - \\widetilde{x} \\over max_x - min_x$  \n",
    "\n",
    "Where\n",
    "- Mean x = $\\sigma$  \n",
    "- Median x = $\\widetilde{x}$  \n",
    "- Maximum value in x = $max_x$  \n",
    "- Minimum value in x = $min_x$  \n",
    "\n",
    "As you may have noticed, the spread of the distribution contributed significantly to the results in question 6.\n",
    "\n",
    "You may find [`np.ptp`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ptp.html) useful for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Have you been using inferential statistics, descriptive statistics, or both?\n",
    "\n",
    "For each exercise, identify the branch of statistics on which you relied for your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population vs. Sample\n",
    "\n",
    "A population is a **whole** group from which a sample is taken.  Generally speaking it is extremely unlikely that you will ever get a perfect populations worth of data so you will have to settle for sampling.  Up untill this point we have assumed that this data **IS** a population.  \n",
    "\n",
    "While this may not seem like a big deal, there are slight adjustments to the way descriptive statistics are counted.  In fact, *Statistics* are used describe samples and *Parameters* are used to describe populations because they are suppose to be the absolute values of a population.\n",
    "\n",
    "Lets randomly sample our data to create a sample of the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Reducing the number of observations\n",
    "\n",
    "It seems likely that this data is a census - that is, the data set includes the entire target population. Suppose that the 506 observations was too much for our computer (as unlikely as this might be) and we needed to pare this down to fewer observations. \n",
    "\n",
    "**9.A Use the `random.sample()` function to select 50 observations from `'AGE'`.**\n",
    "\n",
    "([This documentation](https://docs.python.org/2/library/random.html) may be helpful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age = d['AGE']\n",
    "\n",
    "import random\n",
    "\n",
    "age_sample = random.sample(age, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(age_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.B Identify the type of sampling we just used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 9c. Assess how representative your sample is of the population by comparing the Variance and Standard Deviation of your sample to that of the populations.\n",
    "\n",
    "- It is important to know that Sample measures of spread are calculate with a slight difference from population measures of spread.\n",
    "\n",
    "Population Variance = $\\sum (x_i - \\bar{x})^2  \\over n $  \n",
    "\n",
    "Sample Variance = $\\sum (x_i - \\bar{x})^2  \\over (n-1) $  \n",
    "\n",
    "\n",
    "\n",
    "We divide by $n-1$ to reduce the bias of our sample statistic there for more representative of the true variance of the population.  *In **any** calculation* where we divide by $n$, the calculation for a sample replaces $n$ with $n-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Simple Linear regression model.\n",
    "\n",
    "- Lets build a model that uses one of our variables to predict another.\n",
    "- Once its built we will use the coefficient of determination r2 to assess the error of our models predictions.\n",
    "\n",
    "#### 10a. Choose 2 variables to build your model and assign them to x or y\n",
    "_Using our solution from Q6 `'INDUS'` and `'NOX'` have a decent correlation._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10b. We can all remember the formula for a line being $y = mx +b$.  But how do we find the slope($m$) and the y-int($b$)?\n",
    "\n",
    "### $ m = \\frac{Cov(x,y)}{Var(x)} $\n",
    "\n",
    "_ Slope is equal to the covariance over variance._\n",
    "\n",
    "### $ b = \\bar{y} - (m*\\bar{x})$\n",
    "\n",
    "_ Y intercept is the difference between the mean of y and the product of the slope and the mean of x_\n",
    "\n",
    "\n",
    "You can use [`np.var`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.var.html) to calculate the variance of $x$ and [`np.cov`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html) to calculate the covariance of $x$ and $y$.\n",
    "\n",
    "_Note: The `np.cov` returns a matrix of all possible covariances (x,x),(x,y) & (y,y).  Use `np.cov(x,y)[0][1]` to extract the covariance output of x and y._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# slope\n",
    "# A:\n",
    "\n",
    "# y int\n",
    "# A:\n",
    "\n",
    "#equation\n",
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10c. Use your found slope and y intercept to make a prediction on your original x array.  \n",
    "- Call the output `\"y_pred\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10d. If yor variables are labelled `x`, `y` and `y_pred`, you should be able to run the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# x vs y scatter plot\n",
    "plt.scatter(x,y)\n",
    "plt.show()\n",
    "\n",
    "# X predicting y value\n",
    "plt.scatter(x,y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10e. Calculate the Coefficient of Determination R2 (R-Squared)\n",
    "The formula for R2 is 1 minus the quotient of the Residual Sum of Squares and the Total Sum of Squares. \n",
    "While R2 can be used to generalize the relationship, it is actually a loss metric to evaluate the linear regression being used to explain our variables regression. To put this in a more mathmatical perspective \"How much of the variance of our data is explained with our linear regression\"\n",
    "\n",
    "$$ r2 = 1 - \\frac {RSS} {TSS} $$\n",
    "\n",
    "**Residual Sum of Squares** is a calculation of how off our predictions are from the true value.  Minimulzing this value will cause the calculate of RSS over TSS to decrease thus creating an higher R2 score.\n",
    "\n",
    "_Residual is is basically the difference between what was predicted and what is true._\n",
    "\n",
    " $$ RSS =\\sum\\limits_{i} (y_i - \\widehat{y}  )^2$$\n",
    "\n",
    "- $y_i$ : Y True\n",
    "- $\\widehat{y}$ : Y pred\n",
    "\n",
    "**Total sum of squares.**  This is the variance of true vs pred if we only predicted using the mean of y.  In some ways we can expect this to be the worse predicting model we can create.  This prediction also considered the baseline in many situations.\n",
    "\n",
    " $$ TSS=\\sum\\limits_{i} (y_i - \\bar{y}  )^2 $$\n",
    "\n",
    "- $y_i$ : Y True\n",
    "- $\\bar{y}$ : Mean of all Ys\n",
    "\n",
    "\n",
    "The value of R2 exists on a scale of 1 to negative infinity. Predicting the mean for all values of Y(default model) would create a R2 of 0.  Any R2 less than zero indicates the model (regression) explains less of the variance than the default model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcluate the Residual Sum of Squares (RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcluate the Total Sum of Squares (TSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcluate the Coefficient of Determination (R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
